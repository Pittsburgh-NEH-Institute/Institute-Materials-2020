Week 2:

First slot on Monday:

Two state-of-the-art text models:

tree model
string model

We can move between them. The simplest way to "mark up" a plain text file is word tokenization. 

Plain text has pseudo-markup, but so does XML in the white space between words, for example. 

Some work with plain text doesn't care about subcomponents, e.g., authorship attribution. Other work, e.g., distinguishing dialog from narrative voice, requires structure.

Second slot on Monday:

Third model: TAG (cf. also LMNL, perhaps GODDAG; these were also attempts to understand "what text really is"). Can go from TAG to tree and to plain text; the other way is harder. Markup is making the implicit explicit syntactically; modeling is making it explicit in the ... er ... model. 

LMNL if we can get Luminescent to work. Mention JSON.

Gothenburg model is about modularizing thinking about text, not just about modeling variation, so it's relevant even if there isn't collation/variation. E.g., even with a unique witness, there may be tokenization and normalization in service of linguistic or orthographic analysis.

If you want to make an edition now, the only mature technologies are XML and plain text. 

Transcription is easier with XML than plain text, but tokenization is easier with plain text because you have to flatten the markup. Normalization adds additional information, so that's easier in XML (or JSON). 

Week 2: What's an exercise and what's talking heads? Tokenization and normalization will be part of any work with text; collation will be smaller because it's for variation. This audience already does TEI transcription and markup, so we don't want to do that. 