# Normalization

Computers are very fast at determining whether two strings are identical, which is useful not only for collation, but also for searching, for calculating word frequencies, and for similar operations. What’s more challenging is that there may be variant ways of writing what a human considers (and wants to treat as) the same thing, but because they aren’t string-equal, a computer cannot quickly recognize that they should be regarded as the same. Normalization is the process of telling the computer that strings that are different should nonetheless be regarded as the same. 

Some types of equivalences are challenging for computers because they require real-world knowledge. For example, a speaker of English knows that “Teddy” and “Theodore” may be alternative ways of referring to the same person, but there is no general rule that recognizes nickname ~ full name equivalences. Normalization that recognizes this type of equivalence typically requires a look-up table. Other types of equivalences, though, are algorithmic, that is, able to be expressed through general rules. For example, in normal English orthography words may be separated from one another by white space, and if we don’t care whether the separation, is a space character, a tab, a new line, or some combination thereof (e.g., new line plus several spaces or a tab as indentation before the beginning of a new paragraph), we can systematically  treat all sequences of white-space characters identically.

Normalization is important in the analysis of variation in critical editing. Editors of critical editions typically include only “significant” variants in a critical apparatus, where the definition of “significant” may vary according to the language, the text, and the research goals. Normalization as part of a critical editing process removes non-significant variation and retains significant variation, so that it becomes possible to say which textual witnesses agree or disagree with one another with respect to significant variation, without contamination from insignificant variation.

A simple example of where normalization is useful outside collation is that if you want to know how often a particular word occurs in your English-language text, you probably don’t care whether it’s capitalized (at the beginning of a sentence) or not. To ignore case differences, you might implement a normalization operation that converts everything to lower case, so that you can count forms with and without initial capitalization together. When dealing with differences in capitalization, the normalization could be performed either before or after tokenization, by either lower-casing the entire document before dividing it into words or lower-casing the individual tokens after tokenization. Some types of normalization, though, are most easily performed after tokenization, which is why GM orders normalization after tokenization.

Normalization may be performed on the text being processed or on a shadow copy of the text. If you are counting word frequencies, you might just lowercase the entire text, neutralizing irretrievably the original difference between word tokens with and without initial capitalization. But for the parallel alignment of variants in a critical edition, you might want to retain the original capitalization patterns in your output, while ignoring capitalization for alignment purposes, that is, for determining where two witnesses have what a human would consider “the same reading”. In this case you can create normalized shadow copies, or surrogates, of the tokens, compare the shadow copies to determine where there is and is not significant variation, but output the original, non-normalized forms for eventual visualization.

### White-space normalization

In normal English writing, white space separates words, but much of the time we don’t care about the specific type of white space. For example, by default, [CollateX](https://pypi.python.org/pypi/collatex) ignores trailing whitespace for alignment purposes, so that it will treat “koala” (five characters) as identical to “koala ” (six characters, including a trailing space).

While we might describe this casually by saying that CollateX ignores trailing whitespace during alignment, a more accurate description is that the special treatment of trailing whitespace is the default normalization (Gothenburg stage 2) built into CollateX, that it happens before alignment (stage 3), and that alignment then operates on the normalized shadow copies of the tokens. For this reason, when we tokenize “Faith, Hope, and Charity” and “Faith, Hope and Charity” (with and without the serial comma) in CollateX using the default (built-in) tokenizer, the first sentence has a token “Hope” (four characters), followed by a one-character token that consists of just a comma, and the second has a five-character token “Hope&nbsp;” (with a trailing space). CollateX will regard both as containing a four-character token “Hope” because by default it uses a normalized shadow copy that ignores (normalizes away) trailing white space. 

Similarly, in a plain (without markup) poetic text, we might care about the difference between space characters and new-line characters because that difference is useful for identifying end rhyme. But in a plain prose text, line ends may be an accidental artifact of the size of the printed page, and we might choose to treat words separated by new-line characters identically to words separated by space characters.

### Unicode normalization

[Unicode](http://www.unicode.org) is the system for representing the characters of writing systems in a computer. Representing all characters of all writing systems is a goal that Unicode approaches, but perhaps will never wholly achieve (see, for example, [“I can text you a pile of poo, but I can’t write my name”](https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name)). Unicode is built into all major modern operating systems and applications, and for the most part it just works, and you don’t need to think about it. 

In some situations, though, it is possible to represent the same character (unit of writing) in more than one way. For example, “á” might be encoded as either a single a-with-acute-accent character, or as a sequence of two characters: a regular “a” and a floating acute diacritic that is rendered over it. These representations look the same and are regarded as the same by a human, but because they are not string-equal, a computer that performs only raw string comparison doesn't know that they are informationally identical. Unicode incorporates a process called [Unicode normalization](https://en.wikipedia.org/wiki/Unicode_equivalence), which coverts such variants to a consistent representation. If you regard this type of variability as an inconsistency in your documents, rather than as a feature, you’ll probably want to normalize it away.

### Punctuation normalization

Some editions will care about differences in punctuation, whether between variant witnesses or as part of the genetic process of creating a text. In other cases editors may regard the presence or absence of punctuation, and variation in punctuation characters, as textually insignificant. In this last situation, the editor may use the normalization process to ignore punctuation systematically.

### Normalizing numbers

“Three hundred”, “300”, “CCC”, and “400” are all different from one another, but not in the same way. Some editors will care whether a number is written in words or in digits, or in Arabic or Roman numerals, while others might consider that type of variation insignificant because it does not affect the fundamental meaning of the text. Should alternative lexical representations of the same numerical values be normalized?

### Normalizing for pronunciation

In a writing system with great orthographic instability, it may be desirable to normalize tokens by converting them to phonetic representations and comparing those, instead of comparing the written forms. Such normalizations may wish to take the information load of different parts of the token into consideration. For example, in some writing systems consonants have a higher information load than vowels because vowels are more variable, that is, there are more ways to spell a particular vowel sound than a particular consonsant sound, and there are fewer vowel letters than consonant letters. In languages where words are composed from lexical stems and grammatical endings, the beginning of the word may carry a heavier information load than the end because there are more lexemes than grammatical ending. Phonetic normalization may take information load into account, perhaps retaining only the pronunciation details that are most likely to differentiate meaning.

### Normalization for multilingual alignment

Stage 3 of GM, [alignment](week_2_day_1_alignment.md), is performed on tokens and relies on the assumption that tokens (or their normalized shadow copies) that are similar in composition are candidates for alignment. How this works for texts in the same language is self-evident, but how about multilingual alignment? In some cases (e.g., late Latin and early Italian) there might be enough string similarity to support alignment after aggressive normalization, but how about texts in entirely different writing systems, such as Greek and Church Slavonic? One strategy involves performing (ideally, machine-assisted) linguistic analysis and then using the linguistic information (e.g., part of speech, morphological categories) as the normalized shadows of the tokens. Because there are fewer parts of speech and morphological category values than there are word forms in Greek and Church Slavonic, this simplification increases the number of alignment possibilities and thus complicates finding the correct alignment, but it nonetheless can produce meaningful results if the texts observe reasonably similar word-for-word correspondences and word order.

### Your turn!

What are the normalization challenges in your texts? Which can be treated algorithmically? 